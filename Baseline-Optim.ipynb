{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "187b1286-1b9a-4335-a823-fbf9a7e9dfbb",
   "metadata": {},
   "source": [
    "## Question 2 \n",
    "\n",
    "Using PyTorch,\n",
    "implement a version of the CNN network architecture that was discussed in class, to train and\n",
    "test a handwritten digit recognition system. You can read the paper “Backpropagation Applied to\n",
    "Handwritten Zip Code Recognition” by LeCun et al. 1989 for more details, but your architecture\n",
    "will not follow exactly what was mentioned in the paper. Use the MNIST dataset to train and test\n",
    "the system. Be sure to divide the data into a single training, validation and testing set. Note that\n",
    "you do not need to resize the inputs to size 16 ×16.\n",
    "\n",
    "The baseline system should use the following: Glorot initialization, ReLU activations, mini-batch\n",
    "gradient descent with momentum (β = 0.9), early stopping and a cross-entropy loss function. Use\n",
    "a learning rate scheduler to adjust the learning rate by 10% every 10 epochs, starting with a learn-\n",
    "ing rate of 0.05. You are encouraged to use UITS Carbonate Deep Learning cluster to\n",
    "develop your system, since they have GPUs which will improve the training process.\n",
    "Generate learning curves for the validation and training set. Discuss whether this baseline system\n",
    "overfits, underfits or reasonably fits the validation data. Test this baseline system with the testing\n",
    "data and report the accuracy and show a confusion matrix. Submit your solution to this part\n",
    "of the problem in a Jupyter notebook document named: baseline.ipynb.\n",
    "\n",
    "\n",
    "### Baseline system"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbf3d36d-3d5b-431a-9c20-d1e59aef5165",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Architecture\n",
    "\n",
    "- Layers\n",
    "    - input layer - image (original / scaled)\n",
    "    \n",
    "    - convolution\n",
    "    \n",
    "        -            feature maps\n",
    "        \n",
    "        -                kernal (#glorot initialization)\n",
    "        \n",
    "        -                sliding\n",
    "        \n",
    "    - pooling\n",
    "    \n",
    "        -                  max pooling\n",
    "    \n",
    "    - repeat until flatten\n",
    "    \n",
    "    - feed forward\n",
    "           "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ba437a2e-40b3-454e-9069-334b6181f7b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports as needed for the baseline system\n",
    "\n",
    "import torch\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader, random_split, Subset\n",
    "from torch.utils.data import DataLoader as dl\n",
    "from torch import optim\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "from torch.autograd import Variable\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import seaborn as sn\n",
    "import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8c428bbb-23df-4a34-9789-96797faa21df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This/These cell(s) is responsible for the below mentioned part of the problem statement.\n",
    "\n",
    "# Use the MNIST dataset to train and test the system. \n",
    "# Be sure to divide the data into a single training, validation and testing set. \n",
    "# Note that you do not need to resize the inputs to size 16 ×16.\n",
    "\n",
    "transforms = transforms.Compose([transforms.ToTensor()])\n",
    "training_data = datasets.MNIST(root = 'data', train = True, transform = transforms, download = True)\n",
    "train_ind, valid_ind = random_split(training_data, [50000,10000])\n",
    "train_data = Subset(training_data, train_ind.indices)\n",
    "valid_data = Subset(training_data, valid_ind.indices)\n",
    "test_data = datasets.MNIST(root = 'data', train = False, transform = transforms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "262793b1-7846-40ce-93e6-bd7687f8ff50",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<torch.utils.data.dataset.Subset object at 0x7fc720b347f0> 50000\n",
      "<torch.utils.data.dataset.Subset object at 0x7fc732421ac0> 10000\n",
      "Dataset MNIST\n",
      "    Number of datapoints: 10000\n",
      "    Root location: data\n",
      "    Split: Test\n",
      "    StandardTransform\n",
      "Transform: Compose(\n",
      "               ToTensor()\n",
      "           ) 10000\n"
     ]
    }
   ],
   "source": [
    "print(train_data, len(train_data))\n",
    "print(valid_data, len(valid_data))\n",
    "print(test_data, len(test_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3f44b45e-30f7-41a6-93b6-ea06eb384847",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'train': <torch.utils.data.dataloader.DataLoader at 0x7fc73250f940>,\n",
       " 'valid': <torch.utils.data.dataloader.DataLoader at 0x7fc73250fee0>,\n",
       " 'test': <torch.utils.data.dataloader.DataLoader at 0x7fc73250f490>}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loaders = {}\n",
    "loaders['train'] =  dl(train_data, batch_size = 50, shuffle = False, num_workers=4)\n",
    "loaders['valid'] = dl(valid_data, batch_size = 50, shuffle = False, num_workers=4)\n",
    "loaders['test'] = dl(test_data, batch_size = 50, shuffle = False, num_workers=4)\n",
    "\n",
    "loaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9cd31945-6a14-4d2e-a007-cfa65609f724",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Critical for the baseline \n",
    "### Model architecture turned to code\n",
    "### Layers detailed above\n",
    "\n",
    "### Key implementations in this cell:\n",
    "### Glorot initialization as xavier_uniform\n",
    "### RELU Activation\n",
    "### Convolutions and Pooling\n",
    "\n",
    "class CNN(nn.Module):\n",
    "    def __init__(self):\n",
    "    \n",
    "        # initializing the parent object\n",
    "        super(CNN, self).__init__()\n",
    "        \n",
    "        # creating the first convolution layer\n",
    "        conv_layer1 = nn.Conv2d(1, 32, 5, 1, 2) #in-size, out-size, kernel-size, stride = 1, padding = 2              \n",
    "        # initializing it as per the assignment: Glorot Initialization ~ xavier_uniform\n",
    "        nn.init.xavier_uniform_(conv_layer1.weight)\n",
    "\n",
    "        # only required to perform in a series of steps, only cursory\n",
    "        self.conv1 = nn.Sequential(         \n",
    "            # passing the conv layer variable created above\n",
    "            conv_layer1,                      \n",
    "            # relu layer, as per assignment, however, activation can be different\n",
    "            nn.ReLU(),                      \n",
    "            # taking the generated output from the previous convolution(s) and pooling them \n",
    "            # here, it is a max pooling, that is taking the max output from the convolution filter.\n",
    "            nn.MaxPool2d(2),    \n",
    "        )\n",
    "\n",
    "        conv_layer2 = nn.Conv2d(32, 64, 5, 1, 2)\n",
    "        nn.init.xavier_uniform_(conv_layer2.weight)\n",
    "        self.conv2 = nn.Sequential(         \n",
    "            conv_layer2,     \n",
    "            nn.ReLU(),                      \n",
    "            nn.MaxPool2d(2),                \n",
    "        )\n",
    "        \n",
    "        # fully connected layer, output 10 classes\n",
    "        self.dnn1 = nn.Linear(64 * 7 * 7, 32)\n",
    "        self.out = nn.Linear(32, 10)\n",
    "        \n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.conv2(x)\n",
    "        # flatten the output of conv2 to (batch_size, 32 * 7 * 7)\n",
    "        x = x.view(x.size(0), -1)  \n",
    "        x = self.dnn1(x)\n",
    "        output = self.out(x)\n",
    "        return output, x    # return x for visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "288e25fe-0f58-4e24-ad9a-636af89ffd2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CNN(\n",
      "  (conv1): Sequential(\n",
      "    (0): Conv2d(1, 32, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
      "    (1): ReLU()\n",
      "    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (conv2): Sequential(\n",
      "    (0): Conv2d(32, 64, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
      "    (1): ReLU()\n",
      "    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (dnn1): Linear(in_features=3136, out_features=32, bias=True)\n",
      "  (out): Linear(in_features=32, out_features=10, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "cnn = CNN()\n",
    "print(cnn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0e0c3552-5aef-463c-ba0a-c995efaefe5d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CrossEntropyLoss()"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# cross entropy function as mentioned in the problem statement\n",
    "loss_func = nn.CrossEntropyLoss()   \n",
    "loss_func"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec609055-c25f-4dac-a9f7-57d8e3a4a498",
   "metadata": {},
   "source": [
    "# Nesterov optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "537e00be-d32b-4b65-a9b4-49cf2890d264",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.05\n",
      "Epoch [1/35], Step [50/1000], Loss: 0.0231\n",
      "Epoch [1/35], Step [100/1000], Loss: 0.0056\n",
      "Epoch [1/35], Step [150/1000], Loss: 0.0816\n",
      "Epoch [1/35], Step [200/1000], Loss: 0.0278\n",
      "Epoch [1/35], Step [250/1000], Loss: 0.0558\n",
      "Epoch [1/35], Step [300/1000], Loss: 0.0200\n",
      "Epoch [1/35], Step [350/1000], Loss: 0.1098\n",
      "Epoch [1/35], Step [400/1000], Loss: 0.0283\n",
      "Epoch [1/35], Step [450/1000], Loss: 0.1250\n",
      "Epoch [1/35], Step [500/1000], Loss: 0.0010\n",
      "Epoch [1/35], Step [550/1000], Loss: 0.0156\n",
      "Epoch [1/35], Step [600/1000], Loss: 0.0081\n",
      "Epoch [1/35], Step [650/1000], Loss: 0.0647\n",
      "Epoch [1/35], Step [700/1000], Loss: 0.0384\n",
      "Epoch [1/35], Step [750/1000], Loss: 0.0205\n",
      "Epoch [1/35], Step [800/1000], Loss: 0.0168\n",
      "Epoch [1/35], Step [850/1000], Loss: 0.0351\n",
      "Epoch [1/35], Step [900/1000], Loss: 0.0153\n",
      "Epoch [1/35], Step [950/1000], Loss: 0.0286\n",
      "Epoch [1/35], Step [1000/1000], Loss: 0.0417\n",
      "Validation Accuracy of the model on the 10000 validation images: 1.00\n",
      "0.08142682\n",
      "Epoch [2/35], Step [50/1000], Loss: 0.0068\n",
      "Epoch [2/35], Step [100/1000], Loss: 0.0009\n",
      "Epoch [2/35], Step [150/1000], Loss: 0.0090\n",
      "Epoch [2/35], Step [200/1000], Loss: 0.0052\n",
      "Epoch [2/35], Step [250/1000], Loss: 0.0036\n",
      "Epoch [2/35], Step [300/1000], Loss: 0.0065\n",
      "Epoch [2/35], Step [350/1000], Loss: 0.0219\n",
      "Epoch [2/35], Step [400/1000], Loss: 0.0658\n",
      "Epoch [2/35], Step [450/1000], Loss: 0.0773\n",
      "Epoch [2/35], Step [500/1000], Loss: 0.0009\n",
      "Epoch [2/35], Step [550/1000], Loss: 0.0032\n",
      "Epoch [2/35], Step [600/1000], Loss: 0.0050\n",
      "Epoch [2/35], Step [650/1000], Loss: 0.0142\n",
      "Epoch [2/35], Step [700/1000], Loss: 0.0093\n",
      "Epoch [2/35], Step [750/1000], Loss: 0.0319\n",
      "Epoch [2/35], Step [800/1000], Loss: 0.0029\n",
      "Epoch [2/35], Step [850/1000], Loss: 0.0046\n",
      "Epoch [2/35], Step [900/1000], Loss: 0.0048\n",
      "Epoch [2/35], Step [950/1000], Loss: 0.0243\n",
      "Epoch [2/35], Step [1000/1000], Loss: 0.0286\n",
      "Validation Accuracy of the model on the 10000 validation images: 1.00\n",
      "0.07026594\n",
      "Epoch [3/35], Step [50/1000], Loss: 0.0081\n",
      "Epoch [3/35], Step [100/1000], Loss: 0.0000\n",
      "Epoch [3/35], Step [150/1000], Loss: 0.0061\n",
      "Epoch [3/35], Step [200/1000], Loss: 0.0038\n",
      "Epoch [3/35], Step [250/1000], Loss: 0.0118\n",
      "Epoch [3/35], Step [300/1000], Loss: 0.0115\n",
      "Epoch [3/35], Step [350/1000], Loss: 0.0140\n",
      "Epoch [3/35], Step [400/1000], Loss: 0.0084\n",
      "Epoch [3/35], Step [450/1000], Loss: 0.0044\n",
      "Epoch [3/35], Step [500/1000], Loss: 0.0001\n",
      "Epoch [3/35], Step [550/1000], Loss: 0.0019\n",
      "Epoch [3/35], Step [600/1000], Loss: 0.0003\n",
      "Epoch [3/35], Step [650/1000], Loss: 0.0867\n",
      "Epoch [3/35], Step [700/1000], Loss: 0.0024\n",
      "Epoch [3/35], Step [750/1000], Loss: 0.1033\n",
      "Epoch [3/35], Step [800/1000], Loss: 0.0173\n",
      "Epoch [3/35], Step [850/1000], Loss: 0.0385\n",
      "Epoch [3/35], Step [900/1000], Loss: 0.0007\n",
      "Epoch [3/35], Step [950/1000], Loss: 0.1515\n",
      "Epoch [3/35], Step [1000/1000], Loss: 0.0857\n",
      "Validation Accuracy of the model on the 10000 validation images: 1.00\n",
      "0.06771694\n",
      "Epoch [4/35], Step [50/1000], Loss: 0.0058\n",
      "Epoch [4/35], Step [100/1000], Loss: 0.0000\n",
      "Epoch [4/35], Step [150/1000], Loss: 0.0179\n",
      "Epoch [4/35], Step [200/1000], Loss: 0.0012\n",
      "Epoch [4/35], Step [250/1000], Loss: 0.0976\n",
      "Epoch [4/35], Step [300/1000], Loss: 0.0164\n",
      "Epoch [4/35], Step [350/1000], Loss: 0.0776\n",
      "Epoch [4/35], Step [400/1000], Loss: 0.0218\n",
      "Epoch [4/35], Step [450/1000], Loss: 0.0323\n",
      "Epoch [4/35], Step [500/1000], Loss: 0.0001\n",
      "Epoch [4/35], Step [550/1000], Loss: 0.0000\n",
      "Epoch [4/35], Step [600/1000], Loss: 0.0022\n",
      "Epoch [4/35], Step [650/1000], Loss: 0.0161\n",
      "Epoch [4/35], Step [700/1000], Loss: 0.0004\n",
      "Epoch [4/35], Step [750/1000], Loss: 0.0487\n",
      "Epoch [4/35], Step [800/1000], Loss: 0.0029\n",
      "Epoch [4/35], Step [850/1000], Loss: 0.0005\n",
      "Epoch [4/35], Step [900/1000], Loss: 0.0164\n",
      "Epoch [4/35], Step [950/1000], Loss: 0.0832\n",
      "Epoch [4/35], Step [1000/1000], Loss: 0.1489\n",
      "Validation Accuracy of the model on the 10000 validation images: 0.98\n",
      "0.066390485\n",
      "Epoch [5/35], Step [50/1000], Loss: 0.0058\n",
      "Epoch [5/35], Step [100/1000], Loss: 0.0003\n",
      "Epoch [5/35], Step [150/1000], Loss: 0.0093\n",
      "Epoch [5/35], Step [200/1000], Loss: 0.0009\n",
      "Epoch [5/35], Step [250/1000], Loss: 0.0042\n",
      "Epoch [5/35], Step [300/1000], Loss: 0.0035\n",
      "Epoch [5/35], Step [350/1000], Loss: 0.0162\n",
      "Epoch [5/35], Step [400/1000], Loss: 0.0300\n",
      "Epoch [5/35], Step [450/1000], Loss: 0.1338\n",
      "Epoch [5/35], Step [500/1000], Loss: 0.0007\n",
      "Epoch [5/35], Step [550/1000], Loss: 0.0042\n",
      "Epoch [5/35], Step [600/1000], Loss: 0.0004\n",
      "Epoch [5/35], Step [650/1000], Loss: 0.0513\n",
      "Epoch [5/35], Step [700/1000], Loss: 0.0001\n",
      "Epoch [5/35], Step [750/1000], Loss: 0.0465\n",
      "Epoch [5/35], Step [800/1000], Loss: 0.0008\n",
      "Epoch [5/35], Step [850/1000], Loss: 0.1094\n",
      "Epoch [5/35], Step [900/1000], Loss: 0.0014\n",
      "Epoch [5/35], Step [950/1000], Loss: 0.0026\n",
      "Epoch [5/35], Step [1000/1000], Loss: 0.1799\n",
      "Validation Accuracy of the model on the 10000 validation images: 1.00\n",
      "0.0699709\n",
      "Epoch [6/35], Step [50/1000], Loss: 0.0023\n",
      "Epoch [6/35], Step [100/1000], Loss: 0.0005\n",
      "Epoch [6/35], Step [150/1000], Loss: 0.0283\n",
      "Epoch [6/35], Step [200/1000], Loss: 0.0027\n",
      "Epoch [6/35], Step [250/1000], Loss: 0.0024\n",
      "Epoch [6/35], Step [300/1000], Loss: 0.0074\n",
      "Epoch [6/35], Step [350/1000], Loss: 0.0150\n",
      "Epoch [6/35], Step [400/1000], Loss: 0.0091\n",
      "Epoch [6/35], Step [450/1000], Loss: 0.0128\n",
      "Epoch [6/35], Step [500/1000], Loss: 0.0000\n",
      "Epoch [6/35], Step [550/1000], Loss: 0.0024\n",
      "Epoch [6/35], Step [600/1000], Loss: 0.1027\n",
      "Epoch [6/35], Step [650/1000], Loss: 0.0035\n",
      "Epoch [6/35], Step [700/1000], Loss: 0.0798\n",
      "Epoch [6/35], Step [750/1000], Loss: 0.0770\n",
      "Epoch [6/35], Step [800/1000], Loss: 0.0021\n",
      "Epoch [6/35], Step [850/1000], Loss: 0.0263\n",
      "Epoch [6/35], Step [900/1000], Loss: 0.0134\n",
      "Epoch [6/35], Step [950/1000], Loss: 0.0634\n",
      "Epoch [6/35], Step [1000/1000], Loss: 0.0709\n",
      "Validation Accuracy of the model on the 10000 validation images: 1.00\n",
      "0.06847215\n",
      "Epoch [7/35], Step [50/1000], Loss: 0.0004\n",
      "Epoch [7/35], Step [100/1000], Loss: 0.0023\n",
      "Epoch [7/35], Step [150/1000], Loss: 0.0215\n",
      "Epoch [7/35], Step [200/1000], Loss: 0.0103\n",
      "Epoch [7/35], Step [250/1000], Loss: 0.0025\n",
      "Epoch [7/35], Step [300/1000], Loss: 0.0015\n",
      "Epoch [7/35], Step [350/1000], Loss: 0.1837\n",
      "Epoch [7/35], Step [400/1000], Loss: 0.1523\n",
      "Epoch [7/35], Step [450/1000], Loss: 0.0004\n",
      "Epoch [7/35], Step [500/1000], Loss: 0.0001\n",
      "Epoch [7/35], Step [550/1000], Loss: 0.0017\n",
      "Epoch [7/35], Step [600/1000], Loss: 0.0002\n",
      "Epoch [7/35], Step [650/1000], Loss: 0.0039\n",
      "Epoch [7/35], Step [700/1000], Loss: 0.0001\n",
      "Epoch [7/35], Step [750/1000], Loss: 0.0196\n",
      "Epoch [7/35], Step [800/1000], Loss: 0.0013\n",
      "Epoch [7/35], Step [850/1000], Loss: 0.0012\n",
      "Epoch [7/35], Step [900/1000], Loss: 0.0872\n",
      "Epoch [7/35], Step [950/1000], Loss: 0.0105\n",
      "Epoch [7/35], Step [1000/1000], Loss: 0.0777\n",
      "Validation Accuracy of the model on the 10000 validation images: 1.00\n",
      "0.06781629\n",
      "Epoch [8/35], Step [50/1000], Loss: 0.0105\n",
      "Epoch [8/35], Step [100/1000], Loss: 0.0006\n",
      "Epoch [8/35], Step [150/1000], Loss: 0.0003\n",
      "Epoch [8/35], Step [200/1000], Loss: 0.0013\n",
      "Epoch [8/35], Step [250/1000], Loss: 0.1027\n",
      "Epoch [8/35], Step [300/1000], Loss: 0.0123\n",
      "Epoch [8/35], Step [350/1000], Loss: 0.0636\n",
      "Epoch [8/35], Step [400/1000], Loss: 0.0112\n",
      "Epoch [8/35], Step [450/1000], Loss: 0.0335\n",
      "Epoch [8/35], Step [500/1000], Loss: 0.0075\n",
      "Epoch [8/35], Step [550/1000], Loss: 0.0207\n",
      "Epoch [8/35], Step [600/1000], Loss: 0.0140\n",
      "Epoch [8/35], Step [650/1000], Loss: 0.0008\n",
      "Epoch [8/35], Step [700/1000], Loss: 0.0003\n",
      "Epoch [8/35], Step [750/1000], Loss: 0.0176\n",
      "Epoch [8/35], Step [800/1000], Loss: 0.0042\n",
      "Epoch [8/35], Step [850/1000], Loss: 0.0980\n",
      "Epoch [8/35], Step [900/1000], Loss: 0.0005\n",
      "Epoch [8/35], Step [950/1000], Loss: 0.0958\n",
      "Epoch [8/35], Step [1000/1000], Loss: 0.0016\n",
      "Validation Accuracy of the model on the 10000 validation images: 1.00\n",
      "0.07061311\n",
      "Epoch [9/35], Step [50/1000], Loss: 0.0001\n",
      "Epoch [9/35], Step [100/1000], Loss: 0.0001\n",
      "Epoch [9/35], Step [150/1000], Loss: 0.0043\n",
      "Epoch [9/35], Step [200/1000], Loss: 0.0009\n",
      "Epoch [9/35], Step [250/1000], Loss: 0.0006\n",
      "Epoch [9/35], Step [300/1000], Loss: 0.0100\n",
      "Epoch [9/35], Step [350/1000], Loss: 0.0014\n",
      "Epoch [9/35], Step [400/1000], Loss: 0.0033\n",
      "Epoch [9/35], Step [450/1000], Loss: 0.0019\n",
      "Epoch [9/35], Step [500/1000], Loss: 0.0000\n",
      "Epoch [9/35], Step [550/1000], Loss: 0.0013\n",
      "Epoch [9/35], Step [600/1000], Loss: 0.0002\n",
      "Epoch [9/35], Step [650/1000], Loss: 0.0071\n",
      "Epoch [9/35], Step [700/1000], Loss: 0.0004\n",
      "Epoch [9/35], Step [750/1000], Loss: 0.0243\n",
      "Epoch [9/35], Step [800/1000], Loss: 0.0030\n",
      "Epoch [9/35], Step [850/1000], Loss: 0.0004\n",
      "Epoch [9/35], Step [900/1000], Loss: 0.0004\n",
      "Epoch [9/35], Step [950/1000], Loss: 0.0053\n",
      "Epoch [9/35], Step [1000/1000], Loss: 0.0361\n",
      "Validation Accuracy of the model on the 10000 validation images: 1.00\n",
      "0.06970948\n",
      "Epoch [10/35], Step [50/1000], Loss: 0.0000\n",
      "Epoch [10/35], Step [100/1000], Loss: 0.0000\n",
      "Epoch [10/35], Step [150/1000], Loss: 0.0536\n",
      "Epoch [10/35], Step [200/1000], Loss: 0.0124\n",
      "Epoch [10/35], Step [250/1000], Loss: 0.0394\n",
      "Epoch [10/35], Step [300/1000], Loss: 0.0005\n",
      "Epoch [10/35], Step [350/1000], Loss: 0.0404\n",
      "Epoch [10/35], Step [400/1000], Loss: 0.0056\n",
      "Epoch [10/35], Step [450/1000], Loss: 0.0053\n",
      "Epoch [10/35], Step [500/1000], Loss: 0.0001\n",
      "Epoch [10/35], Step [550/1000], Loss: 0.0001\n",
      "Epoch [10/35], Step [600/1000], Loss: 0.0011\n",
      "Epoch [10/35], Step [650/1000], Loss: 0.0545\n",
      "Epoch [10/35], Step [700/1000], Loss: 0.0000\n",
      "Epoch [10/35], Step [750/1000], Loss: 0.0502\n",
      "Epoch [10/35], Step [800/1000], Loss: 0.0001\n",
      "Epoch [10/35], Step [850/1000], Loss: 0.0001\n",
      "Epoch [10/35], Step [900/1000], Loss: 0.0000\n",
      "Epoch [10/35], Step [950/1000], Loss: 0.0025\n",
      "Epoch [10/35], Step [1000/1000], Loss: 0.0286\n",
      "Validation Accuracy of the model on the 10000 validation images: 1.00\n",
      "0.07132426\n",
      "0.045000000000000005\n",
      "Epoch [11/35], Step [50/1000], Loss: 0.0142\n",
      "Epoch [11/35], Step [100/1000], Loss: 0.0000\n",
      "Epoch [11/35], Step [150/1000], Loss: 0.0168\n",
      "Epoch [11/35], Step [200/1000], Loss: 0.0000\n",
      "Epoch [11/35], Step [250/1000], Loss: 0.0007\n",
      "Epoch [11/35], Step [300/1000], Loss: 0.0558\n",
      "Epoch [11/35], Step [350/1000], Loss: 0.0720\n",
      "Epoch [11/35], Step [400/1000], Loss: 0.0003\n",
      "Epoch [11/35], Step [450/1000], Loss: 0.0288\n",
      "Epoch [11/35], Step [500/1000], Loss: 0.0000\n",
      "Epoch [11/35], Step [550/1000], Loss: 0.0000\n",
      "Epoch [11/35], Step [600/1000], Loss: 0.0000\n",
      "Epoch [11/35], Step [650/1000], Loss: 0.0067\n",
      "Epoch [11/35], Step [700/1000], Loss: 0.0018\n",
      "Epoch [11/35], Step [750/1000], Loss: 0.0019\n",
      "Epoch [11/35], Step [800/1000], Loss: 0.0355\n",
      "Epoch [11/35], Step [850/1000], Loss: 0.0034\n",
      "Epoch [11/35], Step [900/1000], Loss: 0.0018\n",
      "Epoch [11/35], Step [950/1000], Loss: 0.0183\n",
      "Epoch [11/35], Step [1000/1000], Loss: 0.1606\n",
      "Validation Accuracy of the model on the 10000 validation images: 1.00\n",
      "0.0736943\n",
      "Epoch [12/35], Step [50/1000], Loss: 0.0000\n",
      "Epoch [12/35], Step [100/1000], Loss: 0.0000\n",
      "Epoch [12/35], Step [150/1000], Loss: 0.0059\n",
      "Epoch [12/35], Step [200/1000], Loss: 0.0000\n",
      "Epoch [12/35], Step [250/1000], Loss: 0.0765\n",
      "Epoch [12/35], Step [300/1000], Loss: 0.0008\n",
      "Epoch [12/35], Step [350/1000], Loss: 0.0039\n",
      "Epoch [12/35], Step [400/1000], Loss: 0.0003\n",
      "Epoch [12/35], Step [450/1000], Loss: 0.0005\n",
      "Epoch [12/35], Step [500/1000], Loss: 0.0000\n",
      "Epoch [12/35], Step [550/1000], Loss: 0.0000\n",
      "Epoch [12/35], Step [600/1000], Loss: 0.0000\n",
      "Epoch [12/35], Step [650/1000], Loss: 0.0141\n",
      "Epoch [12/35], Step [700/1000], Loss: 0.0000\n",
      "Epoch [12/35], Step [750/1000], Loss: 0.0671\n",
      "Epoch [12/35], Step [800/1000], Loss: 0.0020\n",
      "Epoch [12/35], Step [850/1000], Loss: 0.0142\n",
      "Epoch [12/35], Step [900/1000], Loss: 0.0549\n",
      "Epoch [12/35], Step [950/1000], Loss: 0.0054\n",
      "Epoch [12/35], Step [1000/1000], Loss: 0.0548\n",
      "Validation Accuracy of the model on the 10000 validation images: 1.00\n",
      "0.07545684\n",
      "Epoch [13/35], Step [50/1000], Loss: 0.0288\n",
      "Epoch [13/35], Step [100/1000], Loss: 0.0000\n",
      "Epoch [13/35], Step [150/1000], Loss: 0.0001\n",
      "Epoch [13/35], Step [200/1000], Loss: 0.0344\n",
      "Epoch [13/35], Step [250/1000], Loss: 0.0002\n",
      "Epoch [13/35], Step [300/1000], Loss: 0.0045\n",
      "Epoch [13/35], Step [350/1000], Loss: 0.0015\n",
      "Epoch [13/35], Step [400/1000], Loss: 0.1308\n",
      "Epoch [13/35], Step [450/1000], Loss: 0.0145\n",
      "Epoch [13/35], Step [500/1000], Loss: 0.0000\n",
      "Epoch [13/35], Step [550/1000], Loss: 0.0000\n",
      "Epoch [13/35], Step [600/1000], Loss: 0.0043\n",
      "Epoch [13/35], Step [650/1000], Loss: 0.0005\n",
      "Epoch [13/35], Step [700/1000], Loss: 0.0003\n",
      "Epoch [13/35], Step [750/1000], Loss: 0.0012\n",
      "Epoch [13/35], Step [800/1000], Loss: 0.0000\n",
      "Epoch [13/35], Step [850/1000], Loss: 0.0013\n",
      "Epoch [13/35], Step [900/1000], Loss: 0.0000\n",
      "Epoch [13/35], Step [950/1000], Loss: 0.0678\n",
      "Epoch [13/35], Step [1000/1000], Loss: 0.0003\n",
      "Validation Accuracy of the model on the 10000 validation images: 1.00\n",
      "0.07752594\n",
      "Epoch [14/35], Step [50/1000], Loss: 0.0000\n",
      "Epoch [14/35], Step [100/1000], Loss: 0.0000\n",
      "Epoch [14/35], Step [150/1000], Loss: 0.0094\n",
      "Epoch [14/35], Step [200/1000], Loss: 0.0000\n",
      "Epoch [14/35], Step [250/1000], Loss: 0.0000\n",
      "Epoch [14/35], Step [300/1000], Loss: 0.0114\n",
      "Epoch [14/35], Step [350/1000], Loss: 0.0486\n",
      "Epoch [14/35], Step [400/1000], Loss: 0.0089\n",
      "Epoch [14/35], Step [450/1000], Loss: 0.1175\n",
      "Epoch [14/35], Step [500/1000], Loss: 0.0000\n",
      "Epoch [14/35], Step [550/1000], Loss: 0.0000\n",
      "Epoch [14/35], Step [600/1000], Loss: 0.0000\n",
      "Epoch [14/35], Step [650/1000], Loss: 0.0001\n",
      "Epoch [14/35], Step [700/1000], Loss: 0.0001\n",
      "Epoch [14/35], Step [750/1000], Loss: 0.0023\n",
      "Epoch [14/35], Step [800/1000], Loss: 0.0423\n",
      "Epoch [14/35], Step [850/1000], Loss: 0.0004\n",
      "Epoch [14/35], Step [900/1000], Loss: 0.0607\n",
      "Epoch [14/35], Step [950/1000], Loss: 0.0002\n",
      "Epoch [14/35], Step [1000/1000], Loss: 0.1409\n",
      "Validation Accuracy of the model on the 10000 validation images: 1.00\n",
      "0.07851845\n"
     ]
    }
   ],
   "source": [
    "# as mentioned in the problem statement, \n",
    "# we are taking the SGD optimizer for performing \n",
    "# Nesterov optimizer\n",
    "\n",
    "optimizer = SGD(cnn.parameters(), lr = 0.05, momentum=0.9, nesterov=True)\n",
    "optimizer\n",
    "scheduler = StepLR(optimizer, step_size = 10000, gamma = 0.9)\n",
    "print(optimizer.param_groups[0]['lr'])\n",
    "\n",
    "\n",
    "# Key points from this cell:\n",
    "# Early Stopping\n",
    "# Training\n",
    "# Validation\n",
    "# Learning Rate as per epoch\n",
    "\n",
    "num_epochs = 15\n",
    "loss_tracker = []\n",
    "epoch_tracker = []\n",
    "valid_accuracy_tracker = []\n",
    "valid_loss_tracker =[]\n",
    "\n",
    "patience = 3\n",
    "min_delta = 0.001\n",
    "best_loss = None\n",
    "counter = 0\n",
    "same_min_delta = 0 \n",
    "no_epochs = 0\n",
    "best_model = None\n",
    "\n",
    "def early_stopping_func(val_loss):\n",
    "    global best_loss\n",
    "    global patience\n",
    "    global counter\n",
    "    global same_min_delta\n",
    "    global min_delta\n",
    "    #print(best_loss,val_loss,abs(best_loss - val_loss))\n",
    "    if best_loss == None:\n",
    "        best_loss = val_loss\n",
    "    elif best_loss - val_loss > min_delta:\n",
    "        best_loss = val_loss\n",
    "        # reset counter if validation loss improves\n",
    "        counter = 0\n",
    "    elif abs(best_loss - val_loss) < min_delta:\n",
    "        counter += 1\n",
    "        print(f\"INFO: Early stopping counter {counter} of {patience}\")\n",
    "        if counter >= patience:            \n",
    "            print('INFO: Early stopping')\n",
    "            return  True\n",
    "        else:\n",
    "            return False\n",
    "    \n",
    "def train(num_epochs, cnn, loaders):\n",
    "    \n",
    "    #early_stopping = EarlyStopping(patience=patience, verbose=True)\n",
    "    global no_epochs\n",
    "    cnn.train()\n",
    "    # Train the model\n",
    "    total_step = len(loaders['train'])\n",
    "    for epoch in range(num_epochs):\n",
    "        #############################\n",
    "        ###### Training the model####\n",
    "        #############################\n",
    "        epoch_loss_tracker = []\n",
    "        if epoch % 10 == 0:\n",
    "            print(optimizer.param_groups[0]['lr'])\n",
    "        for _, (images, labels) in enumerate(loaders['train']):\n",
    "            #print(optimizer.param_groups[0]['lr'])\n",
    "            # gives batch data, normalize x when iterate train_loader\n",
    "            image_data = Variable(images) \n",
    "            label_data = Variable(labels) \n",
    "            output = cnn(image_data)[0]\n",
    "            loss = loss_func(output, label_data)\n",
    "            \n",
    "            # clear gradients for this training step   \n",
    "            optimizer.zero_grad()           \n",
    "            # backpropagation, compute gradients \n",
    "            loss.backward()    \n",
    "            # apply gradients             \n",
    "            optimizer.step() \n",
    "            scheduler.step()\n",
    "            if (_+1) % 50 == 0:\n",
    "                print ('Epoch [{}/{}], Step [{}/{}], Loss: {:.4f}'.format(epoch + 1, num_epochs, _ + 1, total_step, loss.item()))\n",
    "            epoch_loss_tracker.append(loss)  \n",
    "        loss_tracker.append(sum(epoch_loss_tracker)/1000)\n",
    "        epoch_tracker.append(epoch)\n",
    "        \n",
    "        ##################################\n",
    "        ###### Fitting on validation #####\n",
    "        ##################################\n",
    "        \n",
    "        \n",
    "        cnn.eval()\n",
    "        valid_loss_batch = []\n",
    "        with torch.no_grad():\n",
    "            correct = 0\n",
    "            total = 0\n",
    "            for images, labels in loaders['valid']:\n",
    "            \n",
    "                image_data = Variable(images) \n",
    "                label_data = Variable(labels) \n",
    "                valid_output, last_layer = cnn(images)\n",
    "                loss = loss_func(valid_output, label_data)\n",
    "                valid_loss_batch.append(loss)\n",
    "\n",
    "                valid_pred_y = torch.max(valid_output, 1)[1].data.squeeze()\n",
    "                accuracy = (valid_pred_y == labels).sum().item() / float(labels.size(0))\n",
    "                valid_accuracy_tracker.append(accuracy)\n",
    "        print('Validation Accuracy of the model on the 10000 validation images: %.2f' % accuracy)\n",
    "        \n",
    "        valid_loss = np.average(np.array(valid_loss_batch))\n",
    "        valid_loss_tracker.append(valid_loss)\n",
    "        \n",
    "        print(valid_loss)\n",
    "        \n",
    "        global best_model \n",
    "        if early_stopping_func(valid_loss):\n",
    "            print(\"Early stopping\")\n",
    "            no_epochs = epoch\n",
    "            best_model = copy.deepcopy(cnn)\n",
    "        \n",
    "train(num_epochs, cnn, loaders)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b80b556b-59f2-4295-a01e-5b32cb81c202",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "x and y must have same first dimension, but have shapes (11,) and (0,)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/sy/2n93h1k14rjbxx8d8rkc53ww0000gn/T/ipykernel_5447/252263874.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mxlabel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"epoch\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mylabel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"loss\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m11\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/matplotlib/pyplot.py\u001b[0m in \u001b[0;36mplot\u001b[0;34m(scalex, scaley, data, *args, **kwargs)\u001b[0m\n\u001b[1;32m   3017\u001b[0m \u001b[0;34m@\u001b[0m\u001b[0m_copy_docstring_and_deprecators\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mAxes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3018\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscalex\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscaley\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3019\u001b[0;31m     return gca().plot(\n\u001b[0m\u001b[1;32m   3020\u001b[0m         \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscalex\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mscalex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscaley\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mscaley\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3021\u001b[0m         **({\"data\": data} if data is not None else {}), **kwargs)\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/matplotlib/axes/_axes.py\u001b[0m in \u001b[0;36mplot\u001b[0;34m(self, scalex, scaley, data, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1603\u001b[0m         \"\"\"\n\u001b[1;32m   1604\u001b[0m         \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcbook\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnormalize_kwargs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmlines\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLine2D\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1605\u001b[0;31m         \u001b[0mlines\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_lines\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1606\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mline\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mlines\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1607\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_line\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mline\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/matplotlib/axes/_base.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, data, *args, **kwargs)\u001b[0m\n\u001b[1;32m    313\u001b[0m                 \u001b[0mthis\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    314\u001b[0m                 \u001b[0margs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 315\u001b[0;31m             \u001b[0;32myield\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_plot_args\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mthis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    316\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    317\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget_next_color\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/matplotlib/axes/_base.py\u001b[0m in \u001b[0;36m_plot_args\u001b[0;34m(self, tup, kwargs, return_kwargs)\u001b[0m\n\u001b[1;32m    499\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    500\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 501\u001b[0;31m             raise ValueError(f\"x and y must have same first dimension, but \"\n\u001b[0m\u001b[1;32m    502\u001b[0m                              f\"have shapes {x.shape} and {y.shape}\")\n\u001b[1;32m    503\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m2\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: x and y must have same first dimension, but have shapes (11,) and (0,)"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Learning curves\n",
    "\n",
    "# Loss vs Epoch\n",
    "# for validation and training\n",
    "\n",
    "loss = [i.data.item() for i in loss_tracker]\n",
    "valid_loss = [i.data.item() for i in valid_loss_tracker]\n",
    "plt.title(\"Learning curve for Training data: Nesterov\")\n",
    "plt.xlabel(\"epoch\")\n",
    "plt.ylabel(\"loss\")\n",
    "plt.plot(np.arange(0,num_epochs),np.array(loss), label = \"Training Loss\")\n",
    "plt.plot(np.arange(0,num_epochs),np.array(valid_loss), label = \"Validation Loss\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "bc3b3add-a2fb-431c-bd11-5ad36e6ddc44",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy of the model on the 10000 test images: 1.00\n",
      "10000 10000\n"
     ]
    }
   ],
   "source": [
    "cnn = copy.deepcopy(best_model)\n",
    "# Testing Accuracy\n",
    "# Added as a part of reconcilation and validation of the model\n",
    "\n",
    "\n",
    "test_labels = []\n",
    "test_pred = []\n",
    "test_accuracy_tracker = []\n",
    "def test():\n",
    "    # Test the model\n",
    "    cnn.eval()\n",
    "    with torch.no_grad():\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        for images, labels in loaders['test']:\n",
    "            #print(labels)\n",
    "            test_labels.append(labels)\n",
    "            test_output, last_layer = cnn(images)\n",
    "            pred_y = torch.max(test_output, 1)[1].data.squeeze()\n",
    "            test_pred.append(pred_y)\n",
    "            accuracy = (pred_y == labels).sum().item() / float(labels.size(0))\n",
    "            test_accuracy_tracker.append(accuracy)\n",
    "    print('Test Accuracy of the model on the 10000 test images: %.2f' % accuracy)\n",
    "\n",
    "test()\n",
    "\n",
    "test_actual = []\n",
    "[test_actual.extend(i.tolist()) for i in test_labels]\n",
    "\n",
    "test_pred_orig = []\n",
    "\n",
    "[test_pred_orig.extend(i.tolist()) for i in test_pred]\n",
    "\n",
    "print(len(test_pred_orig),len(test_actual))\n",
    "\n",
    "# Testing Accuracy\n",
    "# Added as a part of reconcilation and validation of the model\n",
    "\n",
    "\n",
    "plt.title(\"Learning curve for Testing data: Nesterov\")\n",
    "plt.xlabel(\"data\")\n",
    "plt.ylabel(\"accuracy\")\n",
    "plt.plot(np.arange(0,len(loaders['test'])),np.array(test_accuracy_tracker))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a05c0299-58b0-43d2-8c0a-43ba65d6034e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 977    0    1    1    0    2    3    0    2    1]\n",
      " [   0 1132    1    0    0    0    2    3    0    3]\n",
      " [   1    0 1021    0    1    0    0    2    2    0]\n",
      " [   0    2    0 1000    0    4    0    0    1    1]\n",
      " [   0    0    1    0  971    0    2    0    0    5]\n",
      " [   0    0    0    6    0  885    9    0    3    5]\n",
      " [   0    0    0    0    1    1  940    0    0    0]\n",
      " [   0    1    5    0    1    0    0 1017    0    2]\n",
      " [   2    0    3    3    2    0    2    2  966    4]\n",
      " [   0    0    0    0    6    0    0    4    0  988]]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 864x504 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# creating confusion matrix as mentioned in the problem statement\n",
    "\n",
    "cm = confusion_matrix(test_pred_orig,test_actual)\n",
    "print(cm)\n",
    "#producing a scaled version of the confusion matrix\n",
    "df_cm = pd.DataFrame(cm/np.sum(cm) * 10, index = [i for i in range(10)],\n",
    "                     columns = [i for i in range(10)])\n",
    "plt.figure(figsize = (12,7))\n",
    "sn.heatmap(df_cm, annot=True)\n",
    "plt.savefig('output.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8c2581f-7e83-496e-ab63-6b51b3dbbffb",
   "metadata": {},
   "source": [
    "# Adam Optimiser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c115c9ed-d979-456d-bfe1-d494a3511104",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_func = nn.CrossEntropyLoss()   \n",
    "# as mentioned in the problem statement, \n",
    "# we are taking the SGD optimizer for performing \n",
    "# Nesterov optimizer\n",
    "\n",
    "optimizer = Adam(cnn.parameters())\n",
    "optimizer\n",
    "scheduler = StepLR(optimizer, step_size = 10000, gamma = 0.9)\n",
    "print(optimizer.param_groups[0]['lr'])\n",
    "\n",
    "\n",
    "# Key points from this cell:\n",
    "# Early Stopping\n",
    "# Training\n",
    "# Validation\n",
    "# Learning Rate as per epoch\n",
    "\n",
    "num_epochs = 15\n",
    "loss_tracker = []\n",
    "epoch_tracker = []\n",
    "valid_accuracy_tracker = []\n",
    "valid_loss_tracker =[]\n",
    "\n",
    "patience = 3\n",
    "min_delta = 0.001\n",
    "best_loss = None\n",
    "counter = 0\n",
    "same_min_delta = 0 \n",
    "no_epochs = 0\n",
    "best_model = None\n",
    "\n",
    "def early_stopping_func(val_loss):\n",
    "    global best_loss\n",
    "    global patience\n",
    "    global counter\n",
    "    global same_min_delta\n",
    "    global min_delta\n",
    "    #print(best_loss,val_loss,abs(best_loss - val_loss))\n",
    "    if best_loss == None:\n",
    "        best_loss = val_loss\n",
    "    elif best_loss - val_loss > min_delta:\n",
    "        best_loss = val_loss\n",
    "        # reset counter if validation loss improves\n",
    "        counter = 0\n",
    "    elif abs(best_loss - val_loss) < min_delta:\n",
    "        counter += 1\n",
    "        print(f\"INFO: Early stopping counter {counter} of {patience}\")\n",
    "        if counter >= patience:            \n",
    "            print('INFO: Early stopping')\n",
    "            return  True\n",
    "        else:\n",
    "            return False\n",
    "    \n",
    "def train(num_epochs, cnn, loaders):\n",
    "    \n",
    "    #early_stopping = EarlyStopping(patience=patience, verbose=True)\n",
    "    global no_epochs\n",
    "    cnn.train()\n",
    "    # Train the model\n",
    "    total_step = len(loaders['train'])\n",
    "    for epoch in range(num_epochs):\n",
    "        #############################\n",
    "        ###### Training the model####\n",
    "        #############################\n",
    "        epoch_loss_tracker = []\n",
    "        if epoch % 10 == 0:\n",
    "            print(optimizer.param_groups[0]['lr'])\n",
    "        for _, (images, labels) in enumerate(loaders['train']):\n",
    "            #print(optimizer.param_groups[0]['lr'])\n",
    "            # gives batch data, normalize x when iterate train_loader\n",
    "            image_data = Variable(images) \n",
    "            label_data = Variable(labels) \n",
    "            output = cnn(image_data)[0]\n",
    "            loss = loss_func(output, label_data)\n",
    "            \n",
    "            # clear gradients for this training step   \n",
    "            optimizer.zero_grad()           \n",
    "            # backpropagation, compute gradients \n",
    "            loss.backward()    \n",
    "            # apply gradients             \n",
    "            optimizer.step() \n",
    "            scheduler.step()\n",
    "            if (_+1) % 50 == 0:\n",
    "                print ('Epoch [{}/{}], Step [{}/{}], Loss: {:.4f}'.format(epoch + 1, num_epochs, _ + 1, total_step, loss.item()))\n",
    "            epoch_loss_tracker.append(loss)  \n",
    "        loss_tracker.append(sum(epoch_loss_tracker)/1000)\n",
    "        epoch_tracker.append(epoch)\n",
    "        \n",
    "        ##################################\n",
    "        ###### Fitting on validation #####\n",
    "        ##################################\n",
    "        \n",
    "        \n",
    "        cnn.eval()\n",
    "        valid_loss_batch = []\n",
    "        with torch.no_grad():\n",
    "            correct = 0\n",
    "            total = 0\n",
    "            for images, labels in loaders['valid']:\n",
    "            \n",
    "                image_data = Variable(images) \n",
    "                label_data = Variable(labels) \n",
    "                valid_output, last_layer = cnn(images)\n",
    "                loss = loss_func(valid_output, label_data)\n",
    "                valid_loss_batch.append(loss)\n",
    "\n",
    "                valid_pred_y = torch.max(valid_output, 1)[1].data.squeeze()\n",
    "                accuracy = (valid_pred_y == labels).sum().item() / float(labels.size(0))\n",
    "                valid_accuracy_tracker.append(accuracy)\n",
    "        print('Validation Accuracy of the model on the 10000 validation images: %.2f' % accuracy)\n",
    "        \n",
    "        valid_loss = np.average(np.array(valid_loss_batch))\n",
    "        valid_loss_tracker.append(valid_loss)\n",
    "        \n",
    "        print(valid_loss)\n",
    "        \n",
    "        global best_model \n",
    "        if early_stopping_func(valid_loss):\n",
    "            print(\"Early stopping\")\n",
    "            no_epochs = epoch\n",
    "            best_model = copy.deepcopy(cnn)\n",
    "        \n",
    "train(num_epochs, cnn, loaders)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e08f3ebb-3f34-4eed-8d67-7e2b55119055",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Learning curves\n",
    "\n",
    "# Loss vs Epoch\n",
    "# for validation and training\n",
    "\n",
    "loss = [i.data.item() for i in loss_tracker]\n",
    "valid_loss = [i.data.item() for i in valid_loss_tracker]\n",
    "plt.title(\"Learning curve for Training data: Adam\")\n",
    "plt.xlabel(\"epoch\")\n",
    "plt.ylabel(\"loss\")\n",
    "plt.plot(np.arange(0,num_epochs),np.array(loss), label = \"Training Loss\")\n",
    "plt.plot(np.arange(0,num_epochs),np.array(valid_loss), label = \"Validation Loss\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90caa929-634e-49fd-9798-7550f071cf1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn = copy.deepcopy(best_model)\n",
    "# Testing Accuracy\n",
    "# Added as a part of reconcilation and validation of the model\n",
    "\n",
    "\n",
    "test_labels = []\n",
    "test_pred = []\n",
    "test_accuracy_tracker = []\n",
    "def test():\n",
    "    # Test the model\n",
    "    cnn.eval()\n",
    "    with torch.no_grad():\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        for images, labels in loaders['test']:\n",
    "            #print(labels)\n",
    "            test_labels.append(labels)\n",
    "            test_output, last_layer = cnn(images)\n",
    "            pred_y = torch.max(test_output, 1)[1].data.squeeze()\n",
    "            test_pred.append(pred_y)\n",
    "            accuracy = (pred_y == labels).sum().item() / float(labels.size(0))\n",
    "            test_accuracy_tracker.append(accuracy)\n",
    "    print('Test Accuracy of the model on the 10000 test images: %.2f' % accuracy)\n",
    "\n",
    "test()\n",
    "\n",
    "test_actual = []\n",
    "[test_actual.extend(i.tolist()) for i in test_labels]\n",
    "\n",
    "test_pred_orig = []\n",
    "\n",
    "[test_pred_orig.extend(i.tolist()) for i in test_pred]\n",
    "\n",
    "print(len(test_pred_orig),len(test_actual))\n",
    "\n",
    "# Testing Accuracy\n",
    "# Added as a part of reconcilation and validation of the model\n",
    "\n",
    "\n",
    "plt.title(\"Learning curve for Testing data: Adam\")\n",
    "plt.xlabel(\"data\")\n",
    "plt.ylabel(\"accuracy\")\n",
    "plt.plot(np.arange(0,len(loaders['test'])),np.array(test_accuracy_tracker))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de2dc732-ecc6-4436-95e1-3d19ab32b26a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating confusion matrix as mentioned in the problem statement\n",
    "\n",
    "cm = confusion_matrix(test_pred_orig,test_actual)\n",
    "print(cm)\n",
    "#producing a scaled version of the confusion matrix\n",
    "df_cm = pd.DataFrame(cm/np.sum(cm) * 10, index = [i for i in range(10)],\n",
    "                     columns = [i for i in range(10)])\n",
    "plt.figure(figsize = (12,7))\n",
    "sn.heatmap(df_cm, annot=True)\n",
    "plt.savefig('output.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8103bc59-8ea9-40e2-97b3-9dbd46081d6d",
   "metadata": {},
   "source": [
    "# RMSprop Optimiser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d54ba0d-3ec4-4653-96ef-ce4ba69f878d",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_func = nn.CrossEntropyLoss()   \n",
    "\n",
    "\n",
    "# as mentioned in the problem statement, \n",
    "# we are taking the SGD optimizer for performing \n",
    "# Nesterov optimizer\n",
    "\n",
    "optimizer = RMSprop(cnn.parameters())\n",
    "optimizer\n",
    "scheduler = StepLR(optimizer, step_size = 10000, gamma = 0.9)\n",
    "print(optimizer.param_groups[0]['lr'])\n",
    "\n",
    "\n",
    "# Key points from this cell:\n",
    "# Early Stopping\n",
    "# Training\n",
    "# Validation\n",
    "# Learning Rate as per epoch\n",
    "\n",
    "num_epochs = 15\n",
    "loss_tracker = []\n",
    "epoch_tracker = []\n",
    "valid_accuracy_tracker = []\n",
    "valid_loss_tracker =[]\n",
    "\n",
    "patience = 3\n",
    "min_delta = 0.001\n",
    "best_loss = None\n",
    "counter = 0\n",
    "same_min_delta = 0 \n",
    "no_epochs = 0\n",
    "best_model = None\n",
    "\n",
    "def early_stopping_func(val_loss):\n",
    "    global best_loss\n",
    "    global patience\n",
    "    global counter\n",
    "    global same_min_delta\n",
    "    global min_delta\n",
    "    #print(best_loss,val_loss,abs(best_loss - val_loss))\n",
    "    if best_loss == None:\n",
    "        best_loss = val_loss\n",
    "    elif best_loss - val_loss > min_delta:\n",
    "        best_loss = val_loss\n",
    "        # reset counter if validation loss improves\n",
    "        counter = 0\n",
    "    elif abs(best_loss - val_loss) < min_delta:\n",
    "        counter += 1\n",
    "        print(f\"INFO: Early stopping counter {counter} of {patience}\")\n",
    "        if counter >= patience:            \n",
    "            print('INFO: Early stopping')\n",
    "            return  True\n",
    "        else:\n",
    "            return False\n",
    "    \n",
    "def train(num_epochs, cnn, loaders):\n",
    "    \n",
    "    #early_stopping = EarlyStopping(patience=patience, verbose=True)\n",
    "    global no_epochs\n",
    "    cnn.train()\n",
    "    # Train the model\n",
    "    total_step = len(loaders['train'])\n",
    "    for epoch in range(num_epochs):\n",
    "        #############################\n",
    "        ###### Training the model####\n",
    "        #############################\n",
    "        epoch_loss_tracker = []\n",
    "        if epoch % 10 == 0:\n",
    "            print(optimizer.param_groups[0]['lr'])\n",
    "        for _, (images, labels) in enumerate(loaders['train']):\n",
    "            #print(optimizer.param_groups[0]['lr'])\n",
    "            # gives batch data, normalize x when iterate train_loader\n",
    "            image_data = Variable(images) \n",
    "            label_data = Variable(labels) \n",
    "            output = cnn(image_data)[0]\n",
    "            loss = loss_func(output, label_data)\n",
    "            \n",
    "            # clear gradients for this training step   \n",
    "            optimizer.zero_grad()           \n",
    "            # backpropagation, compute gradients \n",
    "            loss.backward()    \n",
    "            # apply gradients             \n",
    "            optimizer.step() \n",
    "            scheduler.step()\n",
    "            if (_+1) % 50 == 0:\n",
    "                print ('Epoch [{}/{}], Step [{}/{}], Loss: {:.4f}'.format(epoch + 1, num_epochs, _ + 1, total_step, loss.item()))\n",
    "            epoch_loss_tracker.append(loss)  \n",
    "        loss_tracker.append(sum(epoch_loss_tracker)/1000)\n",
    "        epoch_tracker.append(epoch)\n",
    "        \n",
    "        ##################################\n",
    "        ###### Fitting on validation #####\n",
    "        ##################################\n",
    "        \n",
    "        \n",
    "        cnn.eval()\n",
    "        valid_loss_batch = []\n",
    "        with torch.no_grad():\n",
    "            correct = 0\n",
    "            total = 0\n",
    "            for images, labels in loaders['valid']:\n",
    "            \n",
    "                image_data = Variable(images) \n",
    "                label_data = Variable(labels) \n",
    "                valid_output, last_layer = cnn(images)\n",
    "                loss = loss_func(valid_output, label_data)\n",
    "                valid_loss_batch.append(loss)\n",
    "\n",
    "                valid_pred_y = torch.max(valid_output, 1)[1].data.squeeze()\n",
    "                accuracy = (valid_pred_y == labels).sum().item() / float(labels.size(0))\n",
    "                valid_accuracy_tracker.append(accuracy)\n",
    "        print('Validation Accuracy of the model on the 10000 validation images: %.2f' % accuracy)\n",
    "        \n",
    "        valid_loss = np.average(np.array(valid_loss_batch))\n",
    "        valid_loss_tracker.append(valid_loss)\n",
    "        \n",
    "        print(valid_loss)\n",
    "        \n",
    "        global best_model \n",
    "        if early_stopping_func(valid_loss):\n",
    "            print(\"Early stopping\")\n",
    "            no_epochs = epoch\n",
    "            best_model = copy.deepcopy(cnn)\n",
    "        \n",
    "train(num_epochs, cnn, loaders)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5836ba5-a96e-468f-9cd3-f5a2ac791c3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Learning curves\n",
    "\n",
    "# Loss vs Epoch\n",
    "# for validation and training\n",
    "\n",
    "loss = [i.data.item() for i in loss_tracker]\n",
    "valid_loss = [i.data.item() for i in valid_loss_tracker]\n",
    "plt.title(\"Learning curve for Training data: RMSprop\")\n",
    "plt.xlabel(\"epoch\")\n",
    "plt.ylabel(\"loss\")\n",
    "plt.plot(np.arange(0,num_epochs),np.array(loss), label = \"Training Loss\")\n",
    "plt.plot(np.arange(0,num_epochs),np.array(valid_loss), label = \"Validation Loss\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f8f03d3-9b4d-4cad-a390-460fcda9a2e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn = copy.deepcopy(best_model)\n",
    "# Testing Accuracy\n",
    "# Added as a part of reconcilation and validation of the model\n",
    "\n",
    "\n",
    "test_labels = []\n",
    "test_pred = []\n",
    "test_accuracy_tracker = []\n",
    "def test():\n",
    "    # Test the model\n",
    "    cnn.eval()\n",
    "    with torch.no_grad():\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        for images, labels in loaders['test']:\n",
    "            #print(labels)\n",
    "            test_labels.append(labels)\n",
    "            test_output, last_layer = cnn(images)\n",
    "            pred_y = torch.max(test_output, 1)[1].data.squeeze()\n",
    "            test_pred.append(pred_y)\n",
    "            accuracy = (pred_y == labels).sum().item() / float(labels.size(0))\n",
    "            test_accuracy_tracker.append(accuracy)\n",
    "    print('Test Accuracy of the model on the 10000 test images: %.2f' % accuracy)\n",
    "\n",
    "test()\n",
    "\n",
    "test_actual = []\n",
    "[test_actual.extend(i.tolist()) for i in test_labels]\n",
    "\n",
    "test_pred_orig = []\n",
    "\n",
    "[test_pred_orig.extend(i.tolist()) for i in test_pred]\n",
    "\n",
    "print(len(test_pred_orig),len(test_actual))\n",
    "\n",
    "# Testing Accuracy\n",
    "# Added as a part of reconcilation and validation of the model\n",
    "\n",
    "\n",
    "plt.title(\"Learning curve for Testing data: RMSprop\")\n",
    "plt.xlabel(\"data\")\n",
    "plt.ylabel(\"accuracy\")\n",
    "plt.plot(np.arange(0,len(loaders['test'])),np.array(test_accuracy_tracker))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bea596e-ba44-4d82-9002-8bc5fcafb1ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating confusion matrix as mentioned in the problem statement\n",
    "\n",
    "cm = confusion_matrix(test_pred_orig,test_actual)\n",
    "print(cm)\n",
    "#producing a scaled version of the confusion matrix\n",
    "df_cm = pd.DataFrame(cm/np.sum(cm) * 10, index = [i for i in range(10)],\n",
    "                     columns = [i for i in range(10)])\n",
    "plt.figure(figsize = (12,7))\n",
    "sn.heatmap(df_cm, annot=True)\n",
    "plt.savefig('output.png')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
